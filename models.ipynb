{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9912863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    show_images_in_grid,\n",
    "    get_weight_vector,\n",
    "    load_state,\n",
    "    save_state,\n",
    "    train_model\n",
    ")\n",
    "\n",
    "from dataset import ExoNetDatasetCnn, ExoNetDatasetV2\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0ea1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x74df877c7750>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define seed for random\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94afbf6b-8dec-40bf-b6cf-56e50ac6e1b1",
   "metadata": {},
   "source": [
    "# Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c21edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories and file paths\n",
    "DATASET_DIR = \"/mnt/f/ExoNet_Images/ExoNet_Images\"\n",
    "DATAFRAMES_DIR = \"/mnt/f/Datasets/Tesis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8c4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_train_df = pd.read_pickle(os.path.join(DATAFRAMES_DIR, \"seq_train_df.pkl\"))\n",
    "seq_val_df = pd.read_pickle(os.path.join(DATAFRAMES_DIR, \"seq_val_df.pkl\"))\n",
    "test_df = pd.read_pickle(os.path.join(DATAFRAMES_DIR, \"test_df.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26aad75-d787-4192-85e4-11d023d32222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 639121 entries, 0 to 639120\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   video     639121 non-null  object\n",
      " 1   frame     639121 non-null  int64 \n",
      " 2   class     639121 non-null  object\n",
      " 3   path      639121 non-null  object\n",
      " 4   sequence  639121 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "seq_train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d22819-d37b-4d18-ad04-dafc81df0a16",
   "metadata": {},
   "source": [
    "# Create transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d7b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "MEAN_DATASET = [0.485, 0.456, 0.406]\n",
    "STD_DATASET = [0.229, 0.224, 0.225]\n",
    "\n",
    "unique_labels = seq_train_df['class'].unique()\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(unique_labels)\n",
    "\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_WIDTH, IMAGE_HEIGHT)),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(\n",
    "        mean=MEAN_DATASET,\n",
    "        std=STD_DATASET\n",
    "    ),\n",
    "    # transforms.ColorJitter(\n",
    "    #     brightness=0.2,\n",
    "    #     contrast=0.2,\n",
    "    #     saturation=0.2,\n",
    "    #     hue=0.05\n",
    "    # ),\n",
    "])\n",
    "\n",
    "val_image_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_WIDTH, IMAGE_HEIGHT)),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(\n",
    "        mean=MEAN_DATASET,\n",
    "        std=STD_DATASET\n",
    "    )\n",
    "])\n",
    "\n",
    "test_image_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_WIDTH, IMAGE_HEIGHT)),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(\n",
    "        mean=MEAN_DATASET,\n",
    "        std=STD_DATASET\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d8117-c321-4af4-99e0-58024483e80c",
   "metadata": {},
   "source": [
    "# Create dataset objects for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe27b10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ExoNetDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Datasets objects\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Sequence length is 15. This value was used to generate sequence of images.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# If the secuence value is modified, the dataset pre-processing must be modified also.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SEQUENCE_LENGTH = 15\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m training_dataset_cnn \u001b[38;5;241m=\u001b[39m \u001b[43mExoNetDataset\u001b[49m(\n\u001b[1;32m      6\u001b[0m     df_labels\u001b[38;5;241m=\u001b[39mseq_train_df,\n\u001b[1;32m      7\u001b[0m     seq_len\u001b[38;5;241m=\u001b[39mSEQUENCE_LENGTH,\n\u001b[1;32m      8\u001b[0m     target_transform\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mtransform,\n\u001b[1;32m      9\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtrain_image_transform\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m validation_dataset_cnn \u001b[38;5;241m=\u001b[39m ExoNetDataset(\n\u001b[1;32m     12\u001b[0m     df_labels\u001b[38;5;241m=\u001b[39mseq_val_df,\n\u001b[1;32m     13\u001b[0m     seq_len\u001b[38;5;241m=\u001b[39mSEQUENCE_LENGTH,\n\u001b[1;32m     14\u001b[0m     target_transform\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mtransform,\n\u001b[1;32m     15\u001b[0m     transform\u001b[38;5;241m=\u001b[39mval_image_transform\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m ExoNetDataset(\n\u001b[1;32m     18\u001b[0m     df_labels\u001b[38;5;241m=\u001b[39mseq_val_df,\n\u001b[1;32m     19\u001b[0m     seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     20\u001b[0m     target_transform\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mtransform,\n\u001b[1;32m     21\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtest_image_transform\n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ExoNetDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Datasets objects\n",
    "# Sequence length is 15. This value was used to generate sequence of images.\n",
    "# If the secuence value is modified, the dataset pre-processing must be modified also.\n",
    "# SEQUENCE_LENGTH = 15\n",
    "training_dataset_cnn = ExoNetDataset(\n",
    "    df_labels=seq_train_df,\n",
    "    seq_len=SEQUENCE_LENGTH,\n",
    "    target_transform=encoder.transform,\n",
    "    transform=train_image_transform\n",
    ")\n",
    "validation_dataset_cnn = ExoNetDataset(\n",
    "    df_labels=seq_val_df,\n",
    "    seq_len=SEQUENCE_LENGTH,\n",
    "    target_transform=encoder.transform,\n",
    "    transform=val_image_transform\n",
    ")\n",
    "test_dataset = ExoNetDataset(\n",
    "    df_labels=seq_val_df,\n",
    "    seq_len=1,\n",
    "    target_transform=encoder.transform,\n",
    "    transform=test_image_transform\n",
    ")\n",
    "\n",
    "print(f\"[Training] Total video chunks of {SEQUENCE_LENGTH} frames: {len(training_dataset)}\")\n",
    "print(f\"[Validation] Total video chunks of {SEQUENCE_LENGTH} frames: {len(validation_dataset)}\")\n",
    "print(f\"[Test] Total video chunks of {1} frames: {len(test_dataset)}\")\n",
    "\n",
    "frames_tensor, labels_tensor = training_dataset[0]\n",
    "print(f\"Frames shape: {frames_tensor.shape}. Labels shape: {labels_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59646a25-eb5e-4eb8-9632-1c3176bd9126",
   "metadata": {},
   "source": [
    "# Train CNN feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df9e47c-5776-44ce-9b39-e099b232897a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are classes equal? Answer: True\n",
      "{'DS-T-LG': 20240, 'DW-S': 24272, 'DW-T-O': 12733, 'IS-S': 12754, 'IS-T-DW': 21516, 'IS-T-LG': 7696, 'LG-S': 65353, 'LG-T-DS': 16212, 'LG-T-DW': 252116, 'LG-T-IS': 18782, 'LG-T-O': 101762, 'LG-T-SE': 85685}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/f/thesis/neural-networks/utils.py:49: UserWarning: Be sure encoder.classes_ and class_value.keys() are the same\n",
      "  warnings.warn(f\"Be sure encoder.classes_ and class_value.keys() are the same\")\n"
     ]
    }
   ],
   "source": [
    "# Checking if encoder.classes_ and classes obtained after applying count method to df are in the same order\n",
    "count_class_train = seq_train_df.groupby(['class'])['frame'].count()\n",
    "classes_from_df = list(count_class_train.to_dict().keys())\n",
    "print(\"Are classes equal? Answer:\", str(classes_from_df).replace(',','') == str(encoder.classes_).replace('\\n',''))\n",
    "print(count_class_train.to_dict())\n",
    "train_loss_weights = get_weight_vector(count_class_train.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8626fb-278a-44e3-9265-3484f930105d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "DS-T-LG     4847\n",
      "DW-S        6834\n",
      "DW-T-O      4436\n",
      "IS-S        2670\n",
      "IS-T-DW     5688\n",
      "IS-T-LG     1756\n",
      "LG-S        9118\n",
      "LG-T-DS     3342\n",
      "LG-T-DW    80693\n",
      "LG-T-IS     3976\n",
      "LG-T-O     29926\n",
      "LG-T-SE    21400\n",
      "Name: frame, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_class_val = seq_val_df.groupby(['class'])['frame'].count()\n",
    "print(count_class_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80fbf15-0f10-4626-a5eb-aa878cbb632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "DS-T-LG     3590\n",
      "DW-S        5601\n",
      "DW-T-O      1923\n",
      "IS-S        1933\n",
      "IS-T-DW     4424\n",
      "IS-T-LG     1588\n",
      "LG-S        3014\n",
      "LG-T-DS     3047\n",
      "LG-T-DW    46280\n",
      "LG-T-IS     3298\n",
      "LG-T-O     21322\n",
      "LG-T-SE    12255\n",
      "Name: frame, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_class_test = test_df.groupby(['class'])['frame'].count()\n",
    "print(count_class_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14485a07-e257-482a-bbd1-dcddea52435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cnn_base(\n",
    "    model,\n",
    "    output_dropout,\n",
    "    device,\n",
    "    num_classes,\n",
    "    in_features,\n",
    "    finetuning=False,\n",
    "    vgg=False\n",
    "):\n",
    "    # Freeze convolutional base\n",
    "    if not finetuning:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Replace final classification head\n",
    "    if not vgg:\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(output_dropout),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        model.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8496d34b-bec2-4aaf-87d3-bc21107061ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient_net\n",
    "efficientnet_base = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "efficientnet_base = setup_cnn_base(\n",
    "    model=efficientnet_base,\n",
    "    output_dropout=0.5,\n",
    "    device=torch.accelerator.current_accelerator(),\n",
    "    num_classes=len(unique_labels),\n",
    "    in_features=efficientnet_base.classifier[1].in_features,\n",
    "    finetuning=True\n",
    ")\n",
    "\n",
    "# mobilenet_v2\n",
    "mobnet_base = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V2)\n",
    "mobnet_base = setup_cnn_base(\n",
    "    model=mobnet_base,\n",
    "    output_dropout=0.5,\n",
    "    device=torch.accelerator.current_accelerator(),\n",
    "    num_classes=len(unique_labels),\n",
    "    in_features=mobnet_base.classifier[1].in_features,\n",
    "    finetuning=True\n",
    ")\n",
    "# VGG16\n",
    "vgg16_base = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "vgg16_base = setup_cnn_base(\n",
    "    model=vgg16_base,\n",
    "    output_dropout=0.5,\n",
    "    device=torch.accelerator.current_accelerator(),\n",
    "    num_classes=len(unique_labels),\n",
    "    in_features=vgg16_base.classifier[6].in_features,\n",
    "    finetuning=True,\n",
    "    vgg=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc9fe5f-cd68-4a10-b95d-8da423b37d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [30/30] - Proc: 1/1:   0%|                                                               | 0/4994 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m     current_epoch \u001b[38;5;241m=\u001b[39m mobnet_checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     42\u001b[0m     current_history \u001b[38;5;241m=\u001b[39m mobnet_checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 45\u001b[0m mobnet_model, mobnet_history, mobnet_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmobnet_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmobnet_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmobnet_val_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmobnet_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmobnet_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmobnet_checkpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmobnet_checkpoint_path_best\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_accelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_history\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/f/thesis/neural-networks/utils.py:126\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, criterion, optimizer, checkpoint_path, checkpoint_path_best, epochs, device, n_processes, i_process, training_type, debug, current_epoch, current_history)\u001b[0m\n\u001b[1;32m    124\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# zero the parameter gradients\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     logits, _, _ \u001b[38;5;241m=\u001b[39m model(xb)\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py:174\u001b[0m, in \u001b[0;36mMobileNetV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py:166\u001b[0m, in \u001b[0;36mMobileNetV2._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Cannot use \"squeeze\" as batch-size can be 1\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py:64\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train MobileNetV2 model\n",
    "mobnet_train_dataset = ExoNetDatasetCnn(\n",
    "    df_labels=seq_train_df,\n",
    "    transform=train_image_transform,\n",
    "    target_transform=encoder.transform\n",
    ")\n",
    "mobnet_val_dataset = ExoNetDatasetCnn(\n",
    "    df_labels=seq_val_df,\n",
    "    transform=val_image_transform,\n",
    "    target_transform=encoder.transform\n",
    ")\n",
    "mobnet_train_dataloader = DataLoader(\n",
    "    mobnet_train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=6\n",
    ")\n",
    "mobnet_val_dataloader = DataLoader(\n",
    "    mobnet_val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=12\n",
    ")\n",
    "\n",
    "lr = 1e-4\n",
    "mobnet_criterion = nn.CrossEntropyLoss(weight=train_loss_weights)\n",
    "mobnet_optimizer = torch.optim.SGD(mobnet_base.parameters(), lr=lr, momentum=0.9)\n",
    "mobnet_checkpoint_path = f\"/mnt/f/Checkpoints/Tesis/cnn/mobnet_base.pth\"\n",
    "mobnet_checkpoint_path_best = f\"/mnt/f/Checkpoints/Tesis/cnn/mobnet_base_best.pth\"\n",
    "\n",
    "mobnet_checkpoint = load_state(mobnet_checkpoint_path, device=torch.accelerator.current_accelerator())\n",
    "current_epoch = 1\n",
    "current_history = None\n",
    "if mobnet_checkpoint:\n",
    "    mobnet_base.load_state_dict(mobnet_checkpoint['model_state_dict'])\n",
    "    mobnet_optimizer.load_state_dict(mobnet_checkpoint['optimizer_state_dict'])\n",
    "    current_epoch = mobnet_checkpoint['current_epoch']\n",
    "    current_history = mobnet_checkpoint['history']\n",
    "\n",
    "\n",
    "mobnet_model, mobnet_history, mobnet_val_loss = train_model(\n",
    "    model=mobnet_base,\n",
    "    train_dataloader=mobnet_train_dataloader,\n",
    "    val_dataloader=mobnet_val_dataloader,\n",
    "    criterion=mobnet_criterion,\n",
    "    optimizer=mobnet_optimizer,\n",
    "    checkpoint_path=mobnet_checkpoint_path,\n",
    "    checkpoint_path_best=mobnet_checkpoint_path_best,\n",
    "    epochs=30,\n",
    "    device=torch.accelerator.current_accelerator(),\n",
    "    debug=False,\n",
    "    current_epoch=current_epoch,\n",
    "    current_history=current_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d7c71b-17e7-447e-8849-51f6c3855d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/30] - Proc: 1/1: 100%|█████████████████████████████| 4994/4994 [24:28<00:00,  3.40it/s, acc=0.6567, loss=0.885]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:34<00:00,  4.08it/s, acc=0.5843, loss=1.49]\n",
      "Epoch [7/30] - Proc: 1/1: 100%|█████████████████████████████| 4994/4994 [23:54<00:00,  3.48it/s, acc=0.6894, loss=0.726]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:51<00:00,  3.88it/s, acc=0.5667, loss=1.59]\n",
      "Epoch [8/30] - Proc: 1/1: 100%|█████████████████████████████| 4994/4994 [24:10<00:00,  3.44it/s, acc=0.7152, loss=0.619]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:51<00:00,  3.88it/s, acc=0.6053, loss=1.77]\n",
      "Epoch [9/30] - Proc: 1/1: 100%|█████████████████████████████| 4994/4994 [24:24<00:00,  3.41it/s, acc=0.7360, loss=0.537]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:57<00:00,  3.82it/s, acc=0.5822, loss=1.87]\n",
      "Epoch [10/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:47<00:00,  3.36it/s, acc=0.7535, loss=0.476]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:51<00:00,  3.88it/s, acc=0.5718, loss=1.97]\n",
      "Epoch [11/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:41<00:00,  3.37it/s, acc=0.7692, loss=0.426]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:54<00:00,  3.85it/s, acc=0.6018, loss=2.19]\n",
      "Epoch [12/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:01<00:00,  3.46it/s, acc=0.7856, loss=0.382]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:58<00:00,  3.81it/s, acc=0.5989, loss=2.22]\n",
      "Epoch [13/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:43<00:00,  3.37it/s, acc=0.7964, loss=0.348]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:56<00:00,  3.83it/s, acc=0.6194, loss=2.36]\n",
      "Epoch [14/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:48<00:00,  3.36it/s, acc=0.8081, loss=0.318]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:58<00:00,  3.81it/s, acc=0.6110, loss=2.49]\n",
      "Epoch [15/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [23:50<00:00,  3.49it/s, acc=0.8178, loss=0.295]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:53<00:00,  3.86it/s, acc=0.6116, loss=2.62]\n",
      "Epoch [16/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:21<00:00,  3.42it/s, acc=0.8276, loss=0.273]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:55<00:00,  3.84it/s, acc=0.6153, loss=2.71]\n",
      "Epoch [17/30] - Proc: 1/1: 100%|█████████████████████████████| 4994/4994 [25:05<00:00,  3.32it/s, acc=0.8368, loss=0.25]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:58<00:00,  3.81it/s, acc=0.6195, loss=2.84]\n",
      "Epoch [18/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:43<00:00,  3.37it/s, acc=0.8442, loss=0.235]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:55<00:00,  3.84it/s, acc=0.6092, loss=2.88]\n",
      "Epoch [19/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:27<00:00,  3.40it/s, acc=0.8506, loss=0.223]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:55<00:00,  3.84it/s, acc=0.6114, loss=2.89]\n",
      "Epoch [20/30] - Proc: 1/1: 100%|█████████████████████████████| 4994/4994 [24:09<00:00,  3.45it/s, acc=0.8582, loss=0.21]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [06:01<00:00,  3.78it/s, acc=0.6087, loss=3.09]\n",
      "Epoch [21/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [26:11<00:00,  3.18it/s, acc=0.8638, loss=0.197]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [06:02<00:00,  3.77it/s, acc=0.6164, loss=3.08]\n",
      "Epoch [22/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [25:50<00:00,  3.22it/s, acc=0.8697, loss=0.188]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:54<00:00,  3.85it/s, acc=0.6173, loss=3.18]\n",
      "Epoch [23/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:30<00:00,  3.40it/s, acc=0.8760, loss=0.174]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:54<00:00,  3.85it/s, acc=0.6238, loss=3.25]\n",
      "Epoch [24/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:26<00:00,  3.41it/s, acc=0.8813, loss=0.165]\n",
      "Validation: 100%|█████████████████████████████████████████████| 1365/1365 [05:50<00:00,  3.90it/s, acc=0.6058, loss=3.2]\n",
      "Epoch [25/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:07<00:00,  3.45it/s, acc=0.8853, loss=0.159]\n",
      "Validation: 100%|█████████████████████████████████████████████| 1365/1365 [05:53<00:00,  3.86it/s, acc=0.6103, loss=3.4]\n",
      "Epoch [26/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:19<00:00,  3.42it/s, acc=0.8897, loss=0.151]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:50<00:00,  3.90it/s, acc=0.6104, loss=3.34]\n",
      "Epoch [27/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:34<00:00,  3.39it/s, acc=0.8919, loss=0.149]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:55<00:00,  3.84it/s, acc=0.6162, loss=3.53]\n",
      "Epoch [28/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:33<00:00,  3.39it/s, acc=0.8978, loss=0.139]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:55<00:00,  3.84it/s, acc=0.6151, loss=3.59]\n",
      "Epoch [29/30] - Proc: 1/1: 100%|████████████████████████████| 4994/4994 [24:03<00:00,  3.46it/s, acc=0.9047, loss=0.129]\n",
      "Validation: 100%|████████████████████████████████████████████| 1365/1365 [05:58<00:00,  3.81it/s, acc=0.6255, loss=3.69]\n",
      "Epoch [30/30] - Proc: 1/1: 100%|█████████████████████████████| 4994/4994 [24:35<00:00,  3.39it/s, acc=0.9089, loss=0.12]\n",
      "Validation: 100%|█████████████████████████████████████████████| 1365/1365 [05:53<00:00,  3.86it/s, acc=0.6264, loss=3.7]\n"
     ]
    }
   ],
   "source": [
    "# Train EfficientNet model\n",
    "efficient_train_dataset = ExoNetDatasetCnn(\n",
    "    df_labels=seq_train_df,\n",
    "    transform=train_image_transform,\n",
    "    target_transform=encoder.transform\n",
    ")\n",
    "# frame, label = efficient_train_dataset[0]\n",
    "# frame = frame.unsqueeze(0)\n",
    "# label = label.unsqueeze(0)\n",
    "# efficientnet_base.eval()\n",
    "# logits = efficientnet_base(frame.to(\"cuda\"))\n",
    "# print(logits.shape, label.shape)\n",
    "efficient_val_dataset = ExoNetDatasetCnn(\n",
    "    df_labels=seq_val_df,\n",
    "    transform=val_image_transform,\n",
    "    target_transform=encoder.transform\n",
    ")\n",
    "efficient_train_dataloader = DataLoader(\n",
    "    efficient_train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=7,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "efficient_val_dataloader = DataLoader(\n",
    "    efficient_val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "lr = 1e-4\n",
    "efficient_criterion = nn.CrossEntropyLoss(weight=train_loss_weights)\n",
    "efficient_optimizer = torch.optim.SGD(efficientnet_base.parameters(), lr=lr, momentum=0.9)\n",
    "efficient_checkpoint_path = f\"/mnt/f/Checkpoints/Tesis/cnn/efficient_net_b0_base.pth\"\n",
    "efficient_checkpoint_path_best = f\"/mnt/f/Checkpoints/Tesis/cnn/efficient_net_b0_base_best.pth\"\n",
    "\n",
    "efficient_checkpoint = load_state(efficient_checkpoint_path, device=torch.accelerator.current_accelerator())\n",
    "if checkpoint:\n",
    "    efficientnet_base.load_state_dict(efficient_checkpoint['model_state_dict'])\n",
    "    efficient_optimizer.load_state_dict(efficient_checkpoint['optimizer_state_dict'])\n",
    "    current_epoch = efficient_checkpoint['current_epoch']\n",
    "    current_history = efficient_checkpoint['history']\n",
    "\n",
    "efficient_model, efficient_history, efficient_val_loss = train_model(\n",
    "    model=efficientnet_base,\n",
    "    train_dataloader=efficient_train_dataloader,\n",
    "    val_dataloader=efficient_val_dataloader,\n",
    "    criterion=efficient_criterion,\n",
    "    optimizer=efficient_optimizer,\n",
    "    checkpoint_path=efficient_checkpoint_path,\n",
    "    checkpoint_path_best=efficient_checkpoint_path_best,\n",
    "    epochs=30,\n",
    "    device=torch.accelerator.current_accelerator(),\n",
    "    debug=False,\n",
    "    current_epoch=current_epoch,\n",
    "    current_history=current_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f635c48-4045-4f0d-8a88-1d0e7de1f3a1",
   "metadata": {},
   "source": [
    "# CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c24b7f-2d0b-4737-a848-b10c6b3dfae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best models\n",
    "best_efficientnet_params = load_state(\"/mnt/f/Checkpoints/Tesis/cnn/efficient_net_b0_base.pth\", device=\"cuda\")\n",
    "best_mobilenet_params = load_state(\"/mnt/f/Checkpoints/Tesis/cnn/mobnet_base_best.pth\", device=\"cuda\")\n",
    "\n",
    "efficientnet_best_model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "efficientnet_best_model = setup_cnn_base(\n",
    "    model=efficientnet_best_model,\n",
    "    output_dropout=0.5,\n",
    "    device=torch.accelerator.current_accelerator(),\n",
    "    num_classes=len(unique_labels),\n",
    "    in_features=efficientnet_best_model.classifier[1].in_features\n",
    ")\n",
    "mobilenet_best_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V2)\n",
    "mobilenet_best_model = setup_cnn_base(\n",
    "    model=mobilenet_best_model,\n",
    "    output_dropout=0.5,\n",
    "    device=torch.accelerator.current_accelerator(),\n",
    "    num_classes=len(unique_labels),\n",
    "    in_features=mobilenet_best_model.classifier[1].in_features\n",
    ")\n",
    "\n",
    "efficientnet_best_model.load_state_dict(best_efficientnet_params[\"model_state_dict\"])\n",
    "mobilenet_best_model.load_state_dict(best_mobilenet_params[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0700d16d-1393-4b21-8cfb-56617597230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cnn-lstm pipeline\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor,\n",
    "        feature_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        num_classes,\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cnn_feature_extractor = feature_extractor\n",
    "        self.cnn_feature_extractor = self.cnn_feature_extractor.to(device)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, h_prev=None, c_prev=None):\n",
    "        \n",
    "        batch_size, seq_len, C, H, W = x.size()\n",
    "\n",
    "        # Flatten for CNN feature extraction\n",
    "        x = x.view(batch_size * seq_len, C, H, W)\n",
    "        self.cnn_feature_extractor.eval()\n",
    "        with torch.no_grad():\n",
    "            features = self.cnn_feature_extractor(x)  # (batch * seq_len, feature_dim)\n",
    "\n",
    "        # Restore sequence dimension\n",
    "        features = features.view(batch_size, seq_len, -1)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        if h_prev and c_prev:\n",
    "            lstm_out, (h, c) = self.lstm(features, (h_prev, c_prev))\n",
    "        else:\n",
    "            lstm_out, (h, c) = self.lstm(features)\n",
    "\n",
    "        # Use last time step for classification\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfb21467-d670-4341-acbe-c64a53ef5a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "cnnlstm = CNN_LSTM(\n",
    "    feature_extractor=mobilenet_best_model.features,\n",
    "    feature_dim=(mobilenet_best_model.classifier[0].in_features * (IMAGE_HEIGHT//32) * (IMAGE_WIDTH//32)),\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    num_classes=len(unique_labels),\n",
    "    device=torch.accelerator.current_accelerator()\n",
    ").to(torch.accelerator.current_accelerator())\n",
    "\n",
    "cnnlstm.eval()\n",
    "x = torch.randn(64,15,3,224,224).to(\"cuda\")\n",
    "output, _, _ = cnnlstm(x)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855e4c1-9afd-4af9-948f-ef5d8e434b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/30] - Proc: 1/1:   0%|                           | 56/19973 [01:04<3:27:05,  1.60it/s, acc=0.9442, loss=0.0729]"
     ]
    }
   ],
   "source": [
    "# training\n",
    "efficientlstm = CNN_LSTM(\n",
    "    feature_extractor=efficientnet_best_model.features,\n",
    "    feature_dim=(efficientnet_best_model.classifier[0].in_features * (IMAGE_HEIGHT//32) * (IMAGE_WIDTH//32)),\n",
    "    hidden_dim=128,\n",
    "    num_layers=1,\n",
    "    num_classes=len(unique_labels),\n",
    "    device=torch.accelerator.current_accelerator()\n",
    ").to(torch.accelerator.current_accelerator())\n",
    "\n",
    "efficientlstm_train_dataset = ExoNetDatasetV2(\n",
    "    df_labels=seq_train_df,\n",
    "    seq_len=15,\n",
    "    transform=train_image_transform,\n",
    "    target_transform=encoder.transform\n",
    ")\n",
    "efficientlstm_val_dataset = ExoNetDatasetV2(\n",
    "    df_labels=seq_val_df,\n",
    "    seq_len=15,\n",
    "    transform=val_image_transform,\n",
    "    target_transform=encoder.transform\n",
    ")\n",
    "efficientlstm_train_dataloader = DataLoader(\n",
    "    efficientlstm_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=7,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "efficientlstm_val_dataloader = DataLoader(\n",
    "    efficientlstm_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "lr = 1e-3\n",
    "efficientlstm_criterion = nn.CrossEntropyLoss(weight=train_loss_weights)\n",
    "efficientlstm_optimizer = torch.optim.SGD(efficientlstm.parameters(), lr=lr, momentum=0.9)\n",
    "efficientlstm_checkpoint_path = f\"/mnt/f/Checkpoints/Tesis/cnn_lstm/efficient_net_b0_base.pth\"\n",
    "efficientlstm_checkpoint_path_best = f\"/mnt/f/Checkpoints/Tesis/cnn_lstm/efficient_net_b0_base_best.pth\"\n",
    "efficientlstm_current_epoch = 1\n",
    "efficientlstm_current_history = None\n",
    "\n",
    "efficientlstm_checkpoint = load_state(efficientlstm_checkpoint_path, device=torch.accelerator.current_accelerator())\n",
    "if efficientlstm_checkpoint:\n",
    "    efficientlstm.load_state_dict(efficientlstm_checkpoint['model_state_dict'])\n",
    "    efficientlstm_optimizer.load_state_dict(efficientlstm_checkpoint['optimizer_state_dict'])\n",
    "    efficientlstm_current_epoch = efficientlstm_checkpoint['current_epoch']\n",
    "    efficientlstm_current_history = efficientlstm_checkpoint['history']\n",
    "\n",
    "efficientlstm_model, efficientlstm_history, efficientlstm_val_loss = train_model(\n",
    "    model=efficientlstm,\n",
    "    train_dataloader=efficientlstm_train_dataloader,\n",
    "    val_dataloader=efficientlstm_val_dataloader,\n",
    "    criterion=efficientlstm_criterion,\n",
    "    optimizer=efficientlstm_optimizer,\n",
    "    checkpoint_path=efficientlstm_checkpoint_path,\n",
    "    checkpoint_path_best=efficientlstm_checkpoint_path_best,\n",
    "    epochs=30,\n",
    "    device=torch.accelerator.current_accelerator(),\n",
    "    debug=False,\n",
    "    current_epoch=efficientlstm_current_epoch,\n",
    "    current_history=efficientlstm_current_history,\n",
    "    training_type=\"lstm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65f250-c97a-4a09-823a-893ad098839e",
   "metadata": {},
   "source": [
    "## ConvLSTM n channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "294a5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        hidden_channels,\n",
    "        kernel_size,\n",
    "        bias=True,\n",
    "        input_dropout=0.0,\n",
    "        recurrent_dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            input_channels + hidden_channels,\n",
    "            4 * hidden_channels,\n",
    "            kernel_size,\n",
    "            padding=padding,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.input_dropout = nn.Dropout2d(input_dropout)\n",
    "        self.recurrent_dropout = nn.Dropout2d(recurrent_dropout)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        if self.input_dropout.p > 0.0:\n",
    "            x = self.input_dropout(x)\n",
    "        if self.recurrent_dropout.p > 0.0:\n",
    "            h = self.recurrent_dropout(h)\n",
    "        \n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        conv_out = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.chunk(conv_out, 4, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a93e9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        hidden_channels_list,\n",
    "        kernel_size,\n",
    "        num_classes,\n",
    "        device,\n",
    "        input_dropout=0.0,\n",
    "        recurrent_dropout=0.0,\n",
    "        output_dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(hidden_channels_list)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            in_channels = input_channels if i == 0 else hidden_channels_list[i - 1]\n",
    "            layers.append(\n",
    "                ConvLSTMCell(\n",
    "                    in_channels,\n",
    "                    hidden_channels_list[i],\n",
    "                    kernel_size[i],\n",
    "                    input_dropout=input_dropout,\n",
    "                    recurrent_dropout=recurrent_dropout\n",
    "                ).to(device)\n",
    "            )\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.avg_pool_2d = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        self.classifier = nn.Linear(hidden_channels_list[-1], num_classes)\n",
    "        self.output_dropout = nn.Dropout2d(output_dropout)\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        # x: (B, T, C, H, W) -> feature maps from EfficientNet\n",
    "        B, T, _, H, W = x.size()\n",
    "\n",
    "        if h is None:\n",
    "            h = [torch.zeros(B, ch, H, W, device=x.device) for ch in self._hidden_channels]\n",
    "            c = [torch.zeros(B, ch, H, W, device=x.device) for ch in self._hidden_channels]\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(T): # For each timestep in the sequence\n",
    "            input_t = x[:, t]\n",
    "\n",
    "            for layer_idx, cell in enumerate(self.layers):\n",
    "                h[layer_idx], c[layer_idx] = cell(input_t, h[layer_idx], c[layer_idx])\n",
    "                input_t = h[layer_idx]\n",
    "            \n",
    "        dropped = self.output_dropout(h[-1])\n",
    "        pooled = self.avg_pool_2d(dropped)  # (B, C_hidden, 1, 1)\n",
    "        pooled = pooled.view(B, -1)  # (B, C_hidden)\n",
    "\n",
    "        frame_output = self.classifier(pooled)  # (B, num_classes)\n",
    "        outputs.append(frame_output)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)  # (B, T, num_classes)\n",
    "        return outputs, h, c\n",
    "\n",
    "    @property\n",
    "    def _hidden_channels(self):\n",
    "        return [cell.hidden_channels for cell in self.layers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d26a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnExtractorConvLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels_list,\n",
    "        kernel_size,\n",
    "        num_classes,\n",
    "        device,\n",
    "        feature_extractor,\n",
    "        feature_dim,\n",
    "        name,\n",
    "        input_dropout,\n",
    "        recurrent_dropout,\n",
    "        output_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_extractor = self.feature_extractor.to(device=device)\n",
    "\n",
    "        # ConvLSTM on top of feature maps\n",
    "        self.convlstm = ConvLSTM(\n",
    "            input_channels=feature_dim,\n",
    "            hidden_channels_list=hidden_channels_list,\n",
    "            kernel_size=kernel_size,\n",
    "            num_classes=num_classes,\n",
    "            input_dropout=input_dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            output_dropout=output_dropout,\n",
    "            device=device\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        # x: (B, T, 3, H, W) raw RGB frames\n",
    "        B, T, C, H, W = x.size()\n",
    "\n",
    "        features = []\n",
    "        for t in range(T):\n",
    "            with torch.no_grad():\n",
    "                f_t = self.feature_extractor(x[:, t])  # (B, 1280, H', W')\n",
    "                features.append(f_t)\n",
    "\n",
    "        features = torch.stack(features, dim=1)  # (B, T, C_feat, H', W')\n",
    "        \n",
    "        out, h_prev, c_prev = self.convlstm(features, h, c)  # (B, T, num_classes)\n",
    "        return out, h_prev, c_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7908ec53-1d62-4c76-98ca-adf230dd7bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(m_test, batch_size, seq_length, channels=3, heigth=224, weigth=224):\n",
    "    m_test.eval()\n",
    "    x = torch.randn(batch_size, seq_length, 3, heigth, weigth).to(\"cuda\")\n",
    "    out, h, c = m_test(x)\n",
    "    out, h, c = m_test(x, h, c)\n",
    "    print(out.shape, len(h), len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f071d-cd98-459b-a21a-ea62c38854f4",
   "metadata": {},
   "source": [
    "## EfficientNetB0 with convLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "872b4b47-2e60-4757-af48-4676cf4a5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_convLSTM = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "efficientnet_convLSTM_dim = 1280\n",
    "efficientnet_name = \"EfficientNetB0_convLSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f26c293c-5d14-4033-9206-274ebae32fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 12]) 2 2\n",
      "torch.Size([1, 1, 12]) 2 2\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "m_test = CnnExtractorConvLSTM(\n",
    "    num_classes=len(unique_labels),\n",
    "    hidden_channels_list=[128, 64],\n",
    "    kernel_size=[(3,3),(3,3)],\n",
    "    device=\"cuda\",\n",
    "    feature_extractor=efficientnet_convLSTM.features,\n",
    "    feature_dim=efficientnet_convLSTM_dim,\n",
    "    name=efficientnet_name,\n",
    "    input_dropout=0.2,\n",
    "    recurrent_dropout=0.1,\n",
    "    output_dropout=0.3\n",
    ").to(device=\"cuda\")\n",
    "test(m_test, 64, SEQUENCE_LENGTH)\n",
    "test(m_test, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba2f46-03c5-4a2d-8fbc-65165e9c8f1d",
   "metadata": {},
   "source": [
    "## MobileNetV2 with convLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b60fc65-21ac-4395-9566-dafa2eb1bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobnet_convLSTM = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "mobnet_convLSTM_dim = 1280\n",
    "mobnet_name = \"MobileNetV2_convLSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf911f96-a081-4afe-9b6e-505aa30de318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 12]) 2 2\n",
      "torch.Size([1, 1, 12]) 2 2\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "m_test = CnnExtractorConvLSTM(\n",
    "    num_classes=len(unique_labels),\n",
    "    hidden_channels_list=[128, 64],\n",
    "    kernel_size=[(3,3),(3,3)],\n",
    "    device=\"cuda\",\n",
    "    feature_extractor=mobnet_convLSTM.features,\n",
    "    feature_dim=mobnet_convLSTM_dim,\n",
    "    name=mobnet_name,\n",
    "    input_dropout=0.2,\n",
    "    recurrent_dropout=0.1,\n",
    "    output_dropout=0.3\n",
    ").to(device=\"cuda\")\n",
    "test(m_test, 64, SEQUENCE_LENGTH)\n",
    "test(m_test, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48469079-dc49-4938-acd3-cc52f0989ca4",
   "metadata": {},
   "source": [
    "## VGG16 with convLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb9afb9b-822c-4e5f-b1f3-570f10394a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_convLSTM = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "vgg16_convLSTM_dim = 512\n",
    "vgg16_name = \"VGG16_convLSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fee1dc10-ea24-436c-a605-d5ee85e25de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 12]) 2 2\n",
      "torch.Size([1, 1, 12]) 2 2\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "m_test = CnnExtractorConvLSTM(\n",
    "    num_classes=len(unique_labels),\n",
    "    hidden_channels_list=[128, 64],\n",
    "    kernel_size=[(3,3),(3,3)],\n",
    "    device=\"cuda\",\n",
    "    feature_extractor=vgg16_convLSTM.features,\n",
    "    feature_dim=vgg16_convLSTM_dim,\n",
    "    name=vgg16_name,\n",
    "    input_dropout=0.2,\n",
    "    recurrent_dropout=0.1,\n",
    "    output_dropout=0.3\n",
    ").to(device=\"cuda\")\n",
    "test(m_test, 64, SEQUENCE_LENGTH)\n",
    "test(m_test, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac86065",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "434f9a58-864f-4500-a39c-649f8b66160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are classes equal? Answer: True\n",
      "{'DS-T-LG': 2063, 'DW-S': 1950, 'DW-T-O': 612, 'IS-S': 1206, 'IS-T-DW': 1615, 'IS-T-LG': 490, 'LG-S': 4640, 'LG-T-DS': 1151, 'LG-T-DW': 28413, 'LG-T-IS': 1147, 'LG-T-O': 5504, 'LG-T-SE': 4555}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/f/thesis/neural-networks/utils.py:49: UserWarning: Be sure encoder.classes_ and class_value.keys() are the same\n",
      "  warnings.warn(f\"Be sure encoder.classes_ and class_value.keys() are the same\")\n"
     ]
    }
   ],
   "source": [
    "# Checking if encoder.classes_ and classes obtained after applying count method to df are in the same order\n",
    "count_class_train = seq_train_df.groupby(['class'])['frame'].count()\n",
    "classes_from_df = list(count_class_train.to_dict().keys())\n",
    "print(\"Are classes equal? Answer:\", str(classes_from_df).replace(',','') == str(encoder.classes_).replace('\\n',''))\n",
    "print(count_class_train.to_dict())\n",
    "train_loss_weights = get_weight_vector(count_class_train.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60a8ab53-aa5b-4984-8d98-ee1373f79087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define params grid\n",
    "batch_sizes = [128]\n",
    "device = \"cuda\"\n",
    "lr = 1e-3\n",
    "params = {\n",
    "    \"batches\": batch_sizes,\n",
    "    \"epochs\": [ 30 ],\n",
    "    \"dropout\": [\n",
    "        [0.2, 0.1, 0.2]\n",
    "    ], # input, recurrent, output\n",
    "    \"layers\": [[128, 64, 32]],\n",
    "}\n",
    "model_names = ['MobileNetV2_convLSTM', 'EfficientNet_convLSTM', 'VGG16_convLSTM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43695a40-d523-41f2-86e8-f7156ee1756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] - Proc: 1/3:   4%|█▎                               | 16/417 [01:31<38:11,  5.71s/it, acc=0.0410, loss=12.2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 95\u001b[0m\n\u001b[1;32m     80\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     81\u001b[0m     train_dataset,\n\u001b[1;32m     82\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mcombo_params[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     88\u001b[0m     val_dataset,\n\u001b[1;32m     89\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mcombo_params[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m model, history, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path_best\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombo_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_results[model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    109\u001b[0m     best_results[model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_loss\n",
      "File \u001b[0;32m/mnt/f/thesis/neural-networks/utils.py:116\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, criterion, optimizer, checkpoint_path, checkpoint_path_best, epochs, device, n_processes, i_process)\u001b[0m\n\u001b[1;32m    113\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    114\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] - Proc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_process\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_processes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (xb, yb) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loop, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    117\u001b[0m     xb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    118\u001b[0m     yb \u001b[38;5;241m=\u001b[39m yb\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1492\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1492\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1444\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1444\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1445\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1446\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1285\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1288\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/miniconda3/envs/ve-tesis/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_params = list(product(model_names, *params.values()))\n",
    "best_results = {\n",
    "    'EfficientNet_convLSTM': {\n",
    "        'val_loss': float('inf'),\n",
    "        'params': None\n",
    "    },\n",
    "    'MobileNetV2_convLSTM': {\n",
    "        'val_loss': float('inf'),\n",
    "        'params': None\n",
    "    },\n",
    "    'VGG16_convLSTM': {\n",
    "        'val_loss': float('inf'),\n",
    "        'params': None\n",
    "    }\n",
    "}\n",
    "checkpoints_prefixes = dict()\n",
    "\n",
    "for i, (model_name, *combo_params) in enumerate(model_params, start=1):\n",
    "    if model_name == 'EfficientNet_convLSTM':\n",
    "        model = CnnExtractorConvLSTM(\n",
    "            num_classes=len(unique_labels),\n",
    "            hidden_channels_list=combo_params[3],\n",
    "            kernel_size=[(3,3) for _ in combo_params[3]],\n",
    "            device=\"cuda\",\n",
    "            feature_extractor=efficientnet_convLSTM.features,\n",
    "            feature_dim=efficientnet_convLSTM_dim,\n",
    "            name=model_name,\n",
    "            input_dropout=combo_params[2][0],\n",
    "            recurrent_dropout=combo_params[2][1],\n",
    "            output_dropout=combo_params[2][2]\n",
    "        ).to(device=\"cuda\")\n",
    "    if model_name == 'MobileNetV2_convLSTM':\n",
    "        model = CnnExtractorConvLSTM(\n",
    "            num_classes=len(unique_labels),\n",
    "            hidden_channels_list=combo_params[3],\n",
    "            kernel_size=[(3,3) for _ in combo_params[3]],\n",
    "            device=\"cuda\",\n",
    "            feature_extractor= mobnet_convLSTM.features,\n",
    "            feature_dim=mobnet_convLSTM_dim,\n",
    "            name=model_name,\n",
    "            input_dropout=combo_params[2][0],\n",
    "            recurrent_dropout=combo_params[2][1],\n",
    "            output_dropout=combo_params[2][2]\n",
    "        ).to(device=\"cuda\")\n",
    "    if model_name == 'VGG16_convLSTM':\n",
    "        model = CnnExtractorConvLSTM(\n",
    "            num_classes=len(unique_labels),\n",
    "            hidden_channels_list=combo_params[3],\n",
    "            kernel_size=[(3,3) for _ in combo_params[3]],\n",
    "            device=\"cuda\",\n",
    "            feature_extractor= vgg16_convLSTM.features,\n",
    "            feature_dim=vgg16_convLSTM_dim,\n",
    "            name=model_name,\n",
    "            input_dropout=combo_params[2][0],\n",
    "            recurrent_dropout=combo_params[2][1],\n",
    "            output_dropout=combo_params[2][2]\n",
    "        ).to(device=\"cuda\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=train_loss_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    checkpoint_prefix = hash(str(combo_params))\n",
    "    checkpoints_prefixes[f'{checkpoint_prefix}'] = combo_params\n",
    "    torch.save(checkpoints_prefixes, f\"/mnt/f/Checkpoints/Tesis/checkpoint_prefixes.pth\")\n",
    "    checkpoint_path = f\"/mnt/f/Checkpoints/Tesis/{model_name}_{checkpoint_prefix}.pth\"\n",
    "    checkpoint_path_best = f\"/mnt/f/Checkpoints/Tesis/{model_name}_{checkpoint_prefix}_best.pth\"\n",
    "    \n",
    "    train_dataset = ExoNetDataset(\n",
    "        df_labels=seq_train_df,\n",
    "        seq_len=SEQUENCE_LENGTH,\n",
    "        target_transform=encoder.transform,\n",
    "        transform=train_image_transform\n",
    "    )\n",
    "    val_dataset = ExoNetDataset(\n",
    "        df_labels=seq_val_df,\n",
    "        seq_len=SEQUENCE_LENGTH,\n",
    "        target_transform=encoder.transform,\n",
    "        transform=val_image_transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=combo_params[0],\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=combo_params[0],\n",
    "        shuffle=False,\n",
    "        num_workers=6,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model, history, val_loss = train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        checkpoint_path_best=checkpoint_path_best,\n",
    "        epochs=combo_params[1],\n",
    "        device=\"cuda\",\n",
    "        n_processes=len(model_params),\n",
    "        i_process=i\n",
    "    )\n",
    "    if val_loss < best_results[model_name]['val_loss']:\n",
    "        best_results[model_name]['val_loss'] = val_loss\n",
    "        best_results[model_name]['params'] = combo_params\n",
    "        \n",
    "    torch.save(best_results, f\"/mnt/f/Checkpoints/Tesis/best_results.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
