{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ea9f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luchocode/miniconda3/envs/ve-tesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import uuid\n",
    "import ollama\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from transformers import CLIPProcessor, CLIPModel, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c275b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_IMG_LABELS = \"/media/luchocode/Extra vol/thesis/data/new_labels/val/labels.csv\"\n",
    "IMG_DIR = \"/media/luchocode/Extra vol/thesis/data/selected_exoimages/val\"\n",
    "PROGRESS_FILE = \"/media/luchocode/Extra vol/thesis/data/new_labels/val/llava_progress.json\"\n",
    "FRAMES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9684c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(row, root_dir):\n",
    "    filename = f\"['{row['video']}'] frame {row['frame']}.jpg\"\n",
    "    return os.path.join(root_dir, row['class'], filename)\n",
    "\n",
    "def transform_image(img_path):\n",
    "    raw_image = Image.open(img_path).convert('RGB')\n",
    "    vis_transform = T.Compose([\n",
    "        T.Resize(256),\n",
    "        T.CenterCrop(224)\n",
    "    ])\n",
    "    model_view_image = vis_transform(raw_image)\n",
    "    return model_view_image\n",
    "\n",
    "\n",
    "# Create an image appending 6 frames horizontally\n",
    "def create_context_strip(image_paths):\n",
    "    if len(image_paths) != 6:\n",
    "        raise ValueError(f\"Must provide exactly 6 image paths. Current len: {len(image_paths)}\")\n",
    "\n",
    "    images = [transform_image(p) for p in image_paths]\n",
    "    \n",
    "    tile_size = 224\n",
    "    # Resize all to match the first image's height for consistency\n",
    "    base_w, base_h = images[0].size\n",
    "    target_w = int(base_w * (tile_size / base_h))\n",
    "    \n",
    "    resized_imgs = [img.resize((tile_size, tile_size)) for img in images]\n",
    "    \n",
    "    # Create Blank Canvas (3 cols wide, 2 rows high)\n",
    "    grid_w = tile_size * 3\n",
    "    grid_h = tile_size * 2\n",
    "    grid_img = Image.new('RGB', (grid_w, grid_h))\n",
    "    \n",
    "    # Paste them in\n",
    "    # Row 1\n",
    "    grid_img.paste(resized_imgs[0], (0, 0))\n",
    "    grid_img.paste(resized_imgs[1], (tile_size, 0))\n",
    "    grid_img.paste(resized_imgs[2], (tile_size * 2, 0))\n",
    "    \n",
    "    # Row 2\n",
    "    grid_img.paste(resized_imgs[3], (0, tile_size))\n",
    "    grid_img.paste(resized_imgs[4], (tile_size, tile_size))\n",
    "    grid_img.paste(resized_imgs[5], (tile_size * 2, tile_size)) # Target Frame\n",
    "\n",
    "    return grid_img\n",
    "\n",
    "\n",
    "def get_new_label(img):\n",
    "    prompt = \"\"\"\n",
    "    You are an autonomous exoskeleton vision system.\n",
    "    The image is a \"History Grid\" of 6 frames (Order: Top-Left to Bottom-Right).\n",
    "    - Frames 1-5: Past history.\n",
    "    - Frame 6 (Bottom-Right): The CURRENT VIEW.\n",
    "\n",
    "    YOUR TASK: Classify the \"Locomotion Mode\" for Frame 6.\n",
    "\n",
    "    ### THE 12 EXONET CLASSES:\n",
    "    1. LG-S     (Level Ground - Steady)\n",
    "    2. LG-T-IS  (Level Ground -> Transition to Incline Stairs)\n",
    "    3. LG-T-DS  (Level Ground -> Transition to Decline Stairs)\n",
    "    4. LG-T-DW  (Level Ground -> Transition to Door/Wall)\n",
    "    5. LG-T-O   (Level Ground -> Transition to Obstacle)\n",
    "    6. IS-S     (Incline Stairs - Steady)\n",
    "    7. IS-T-LG  (Incline Stairs -> Transition to Level Ground)\n",
    "    8. IS-T-DW  (Incline Stairs -> Transition to Door/Wall)\n",
    "    9. DS-S     (Decline Stairs - Steady)\n",
    "    10. DS-T-LG (Decline Stairs -> Transition to Level Ground)\n",
    "    11. DS-T-DW (Decline Stairs -> Transition to Door/Wall)\n",
    "    12. DW-S    (Door/Wall - Steady)\n",
    "\n",
    "    ### DECISION LOGIC:\n",
    "    - Focus on Frame 6.\n",
    "    - If the terrain is consistent for 2 meters -> STEADY (-S).\n",
    "    - If a NEW terrain (Stairs/Door) is within 2 steps -> TRANSITION (-T-).\n",
    "\n",
    "    Return ONLY the exact Label Code.\n",
    "    \"\"\"\n",
    "    OLLAMA_OPTIONS = {\n",
    "        \"num_ctx\": 100000, \n",
    "        \"temperature\": 0  # 0 makes the model strictly deterministic (good for labels)\n",
    "    }\n",
    "    exonet_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"label\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The ExoNet classification code\",\n",
    "            \"enum\": [\n",
    "                \"LG-S\", \"LG-T-IS\", \"LG-T-DS\", \"LG-T-DW\", \"LG-T-O\",\n",
    "                \"IS-S\", \"IS-T-LG\", \"IS-T-DW\",\n",
    "                \"DS-S\", \"DS-T-LG\", \"DS-T-DW\",\n",
    "                \"DW-S\"\n",
    "            ]\n",
    "        },\n",
    "        \"reasoning\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Brief explanation of why this label was chosen based on visual evidence\"\n",
    "        },\n",
    "        \"confidence\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Confidence score between 0.0 and 1.0\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"label\", \"reasoning\", \"confidence\"]\n",
    "}\n",
    "    response = ollama.chat(model='llava', messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "            'images': [img]\n",
    "        },\n",
    "    ], options=OLLAMA_OPTIONS, format=exonet_schema)\n",
    "    return json.loads(response['message']['content'].strip())\n",
    "\n",
    "\n",
    "def store_progress(frame, video, idx):\n",
    "    data = {\"frame\": int(frame), \"video\": video, \"idx\": idx}\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def process_frames(path_to_images):\n",
    "\n",
    "    df = pd.read_csv(path_to_images)\n",
    "    videos = df[\"video\"].unique()\n",
    "    \n",
    "    for idx_video, video in enumerate(videos, start=1):\n",
    "        print(f\"\\nVideo ({idx_video}/{len(videos)}): '{video}'\")\n",
    "\n",
    "        df_video:pd.DataFrame = df[df['video']==video].reset_index()\n",
    "\n",
    "        print(f\"Frames to process: {len(df_video)}\")\n",
    "\n",
    "        img_buffer = []\n",
    "        for idx in range(len(df_video)):\n",
    "            row = df_video.iloc[idx]\n",
    "            image_path = get_image_path(row, IMG_DIR)\n",
    "            \n",
    "            if len(img_buffer) == 0:\n",
    "                img_buffer = [image_path for _ in range(FRAMES)]\n",
    "            \n",
    "            else:\n",
    "                img_buffer.pop(0)\n",
    "                img_buffer.append(image_path)\n",
    "\n",
    "            grid_image = create_context_strip(img_buffer)\n",
    "            grid_image.save(\"grid_image.jpg\", format='JPEG', quality=95)\n",
    "            new_label_assignation = get_new_label(\"grid_image.jpg\")\n",
    "            \n",
    "            index = int(row[\"index\"])\n",
    "            df.at[index, \"new_class\"] = new_label_assignation[\"label\"]\n",
    "            df.to_csv(path_to_images, index=False)\n",
    "\n",
    "            store_progress(frame=row[\"frame\"], video=video, idx=index)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                print(f\"Frames processed: {idx}/{len(df)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "CSV_PATH = \"/media/luchocode/Extra vol/thesis/data/new_labels/val/labels.csv\"\n",
    "TEMP_DIR = \"temp_grids\"               # Directory for grid images\n",
    "MAX_WORKERS = 8                       # Set to 4 for your 32GB VRAM (allows 4 parallel Ollama requests)\n",
    "SAVE_INTERVAL = 50                    # Save CSV every 50 processed rows\n",
    "\n",
    "# Creates temp folder if it doesn't exist\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def get_image_path(row, root_dir):\n",
    "    # Adjust this logic to match your actual file structure\n",
    "    filename = f\"['{row['video']}'] frame {row['frame']}.jpg\"\n",
    "    return os.path.join(root_dir, row['class'], filename)\n",
    "\n",
    "def transform_image(img_path):\n",
    "    try:\n",
    "        raw_image = Image.open(img_path).convert('RGB')\n",
    "        vis_transform = T.Compose([\n",
    "            T.Resize(256),\n",
    "            T.CenterCrop(224)\n",
    "        ])\n",
    "        return vis_transform(raw_image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")\n",
    "        # Return black square on failure to prevent crash\n",
    "        return Image.new('RGB', (224, 224))\n",
    "\n",
    "def create_context_strip(image_paths):\n",
    "    if len(image_paths) != 6:\n",
    "        raise ValueError(f\"Must provide exactly 6 image paths. Current len: {len(image_paths)}\")\n",
    "\n",
    "    images = [transform_image(p) for p in image_paths]\n",
    "    \n",
    "    tile_size = 224\n",
    "    grid_w = tile_size * 3\n",
    "    grid_h = tile_size * 2\n",
    "    grid_img = Image.new('RGB', (grid_w, grid_h))\n",
    "    \n",
    "    # Row 1\n",
    "    grid_img.paste(images[0], (0, 0))\n",
    "    grid_img.paste(images[1], (tile_size, 0))\n",
    "    grid_img.paste(images[2], (tile_size * 2, 0))\n",
    "    \n",
    "    # Row 2\n",
    "    grid_img.paste(images[3], (0, tile_size))\n",
    "    grid_img.paste(images[4], (tile_size, tile_size))\n",
    "    grid_img.paste(images[5], (tile_size * 2, tile_size)) # Target Frame\n",
    "\n",
    "    return grid_img\n",
    "\n",
    "def get_context_window(df, current_idx):\n",
    "    \"\"\"\n",
    "    Stateless function to get the 6 image paths for a specific row index.\n",
    "    Handles padding if we are at the start of a video.\n",
    "    \"\"\"\n",
    "    current_row = df.loc[current_idx]\n",
    "    video_id = current_row['video']\n",
    "    \n",
    "    # Get previous 5 indices + current index\n",
    "    # We look back 5 steps, but we must stay within the bounds of the dataframe\n",
    "    start_lookback = max(0, current_idx - 5)\n",
    "    \n",
    "    potential_rows = df.loc[start_lookback : current_idx]\n",
    "    \n",
    "    # Filter to ensure we only get frames from the SAME video\n",
    "    # (In case the dataframe has multiple videos concatenated)\n",
    "    same_video_rows = potential_rows[potential_rows['video'] == video_id]\n",
    "    \n",
    "    paths = [get_image_path(row, IMG_DIR) for _, row in same_video_rows.iterrows()]\n",
    "    \n",
    "    # Padding Logic:\n",
    "    # If we are at frame 0, we have 1 path. We need 6.\n",
    "    # We pad the START with the first available frame (replicating your buffer logic)\n",
    "    while len(paths) < 6:\n",
    "        paths.insert(0, paths[0])\n",
    "        \n",
    "    return paths\n",
    "\n",
    "def get_new_label(img_path):\n",
    "    prompt = \"\"\"\n",
    "    You are the vision system for a robotic exoskeleton.\n",
    "    \n",
    "    ### INPUT DATA\n",
    "    The image is a composite grid of 6 frames showing the user's movement history.\n",
    "    - **Layout:** 2 Rows x 3 Columns.\n",
    "    - **Sequence:** Frame 1 (Top-Left) is the oldest. Frame 6 (Bottom-Right) is the CURRENT VIEW.\n",
    "    \n",
    "    ### TASK\n",
    "    Analyze **Frame 6 (Bottom-Right)** to classify the user's current locomotion mode.\n",
    "    \n",
    "    ### THE 12 EXONET CLASSES\n",
    "    1. LG-S     (Level Ground - Steady)\n",
    "    2. LG-T-IS  (Level Ground -> Transition to Incline Stairs)\n",
    "    3. LG-T-DS  (Level Ground -> Transition to Decline Stairs)\n",
    "    4. LG-T-DW  (Level Ground -> Transition to Door/Wall)\n",
    "    5. LG-T-O   (Level Ground -> Transition to Obstacle)\n",
    "    6. IS-S     (Incline Stairs - Steady)\n",
    "    7. IS-T-LG  (Incline Stairs -> Transition to Level Ground)\n",
    "    8. IS-T-DW  (Incline Stairs -> Transition to Door/Wall)\n",
    "    9. DS-S     (Decline Stairs - Steady)\n",
    "    10. DS-T-LG (Decline Stairs -> Transition to Level Ground)\n",
    "    11. DS-T-DW (Decline Stairs -> Transition to Door/Wall)\n",
    "    12. DW-S    (Door/Wall - Steady)\n",
    "\n",
    "    ### GEOMETRIC DECISION LOGIC (The \"2-Meter Rule\")\n",
    "    Look at the floor in Frame 6. Estimate the path for the next 2 meters (approx 2 human steps).\n",
    "    - **STEADY (-S):** The terrain type (flat, stairs) remains exactly the same for the next 2 meters.\n",
    "    - **TRANSITION (-T-):** A *new* terrain feature (e.g., the first step of a staircase, a door threshold) is visible and will be reached within 2 meters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # JSON Schema\n",
    "    exonet_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"label\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\n",
    "                    \"LG-S\", \"LG-T-IS\", \"LG-T-DS\", \"LG-T-DW\", \"LG-T-O\",\n",
    "                    \"IS-S\", \"IS-T-LG\", \"IS-T-DW\",\n",
    "                    \"DS-S\", \"DS-T-LG\", \"DS-T-DW\",\n",
    "                    \"DW-S\"\n",
    "                ]\n",
    "            },\n",
    "            \"reasoning\": {\"type\": \"string\"},\n",
    "            \"confidence\": {\"type\": \"number\"}\n",
    "        },\n",
    "        \"required\": [\"label\", \"reasoning\", \"confidence\"]\n",
    "    }\n",
    "    \n",
    "    OLLAMA_OPTIONS = {\n",
    "        \"num_ctx\": 16384, # 16k is safer/faster than 100k and plenty for 6 frames\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='minicpm-v:latest', \n",
    "            messages=[{'role': 'user', 'content': prompt, 'images': [img_path]}],\n",
    "            options=OLLAMA_OPTIONS, \n",
    "            format=exonet_schema\n",
    "        )\n",
    "        return json.loads(response['message']['content'])\n",
    "    except Exception as e:\n",
    "        # print(f\"Ollama Error: {e}\") \n",
    "        return None\n",
    "\n",
    "# --- WORKER FUNCTION ---\n",
    "\n",
    "def process_single_row(index, df_ref):\n",
    "    \"\"\"\n",
    "    Everything needed to process ONE row independently.\n",
    "    Returns: (index, label_data)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Get Context Paths (Stateless)\n",
    "        paths = get_context_window(df_ref, index)\n",
    "        \n",
    "        # 2. Create Grid\n",
    "        grid = create_context_strip(paths)\n",
    "        \n",
    "        # 3. Save to Unique Temp File (Thread Safety)\n",
    "        unique_name = f\"grid_{index}_{uuid.uuid4().hex[:6]}.jpg\"\n",
    "        temp_path = os.path.join(TEMP_DIR, unique_name)\n",
    "        grid.save(temp_path, format='JPEG', quality=95)\n",
    "        \n",
    "        # 4. Inference\n",
    "        result = get_new_label(temp_path)\n",
    "        \n",
    "        # 5. Cleanup\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "            \n",
    "        return index, result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Row {index} failed: {e}\")\n",
    "        return index, None\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "def process_frames_parallel(csv_path):\n",
    "    print(f\"Loading Dataset: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Filter for rows that haven't been processed yet (optional)\n",
    "    # if 'new_class' in df.columns:\n",
    "    #     to_process = df[df['new_class'].isna()]\n",
    "    # else:\n",
    "    to_process = df\n",
    "\n",
    "    print(f\"Starting Parallel Processing on {len(to_process)} frames...\")\n",
    "    print(f\"Workers: {MAX_WORKERS}\")\n",
    "    \n",
    "    # Dictionary to hold results temporarily\n",
    "    results_cache = {}\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        # We pass 'df' (read-only) to every worker\n",
    "        future_to_idx = {\n",
    "            executor.submit(process_single_row, idx, df): idx \n",
    "            for idx in to_process.index\n",
    "        }\n",
    "        \n",
    "        # Process as they complete\n",
    "        for i, future in tqdm(enumerate(as_completed(future_to_idx)), total=len(future_to_idx)):\n",
    "            idx = future_to_idx[future]\n",
    "            idx, data = future.result()\n",
    "            \n",
    "            if data and \"label\" in data:\n",
    "                # Update our cache\n",
    "                results_cache[idx] = data[\"label\"]\n",
    "                \n",
    "                # Update DataFrame immediately (safe in main thread)\n",
    "                df.at[idx, \"new_class\"] = data[\"label\"]\n",
    "                # Optional: Store confidence/reasoning if you want\n",
    "                # df.at[idx, \"reason\"] = data[\"reasoning\"] \n",
    "            else:\n",
    "                print(f\"Failed to get label for index {idx}\")\n",
    "\n",
    "            # Save periodically (e.g., every 50 frames)\n",
    "            if i % SAVE_INTERVAL == 0 and i > 0:\n",
    "                df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Final Save\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Processing Complete. Saved to CSV.\")\n",
    "    \n",
    "    # Clean up temp dir\n",
    "    try:\n",
    "        os.rmdir(TEMP_DIR)\n",
    "    except:\n",
    "        pass # Directory likely not empty or permission issue, ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3460c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset: /media/luchocode/Extra vol/thesis/data/new_labels/val/labels.csv\n",
      "Starting Parallel Processing on 181087 frames...\n",
      "Workers: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 912/181087 [20:09<66:23:56,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# start labeling process\n",
    "process_frames_parallel(PATH_TO_IMG_LABELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve-tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
