{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57bb06cb-ca0a-49ea-bbe4-c51f1dc7f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "193080b1-76e3-4ea7-985b-787a068aaf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a8fdfe1950>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define seed for random\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fde423a8-0f8a-4f36-8f6a-63cacd4ace02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories and file paths\n",
    "DATASET_DIR = Path(\"F:/ExoNet_Images/ExoNet_Images\")\n",
    "DATAFRAMES_DIR = Path(\"F:/Datasets/Tesis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8db27a98-f787-4848-831b-960976b070eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108275 entries, 0 to 108274\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   video   108275 non-null  object\n",
      " 1   frame   108275 non-null  int64 \n",
      " 2   class   108275 non-null  object\n",
      " 3   exist   108275 non-null  bool  \n",
      " 4   path    108275 non-null  object\n",
      "dtypes: bool(1), int64(1), object(3)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_pickle(os.path.join(DATAFRAMES_DIR, \"test_df.pkl\"))\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844cf7c7-3ee3-4aed-80ab-76f16cad3488",
   "metadata": {},
   "source": [
    "Usando foundational models para evaluar que tan bien funcionan para clasificar los frames de los videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99fe725a-3301-4bb8-bf27-83cce0c208c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ExoNetDataset\n",
    "\n",
    "# Transforms\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "MEAN_DATASET = [0.485, 0.456, 0.406]\n",
    "STD_DATASET = [0.229, 0.224, 0.225]\n",
    "\n",
    "unique_labels = test_df['class'].unique()\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(unique_labels)\n",
    "\n",
    "test_image_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_WIDTH, IMAGE_HEIGHT)),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(\n",
    "        mean=MEAN_DATASET,\n",
    "        std=STD_DATASET\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a84c8ca5-b03b-4f81-9e1c-256262974aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Total video chunks of 1 frames: 108275\n",
      "Frames shape: torch.Size([1, 3, 224, 224]). Labels shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Datasets objects\n",
    "SEQUENCE_LENGTH = 1\n",
    "test_dataset = ExoNetDataset(\n",
    "    df_labels=test_df,\n",
    "    img_dir=DATASET_DIR,\n",
    "    seq_len=SEQUENCE_LENGTH,\n",
    "    target_transform=encoder.transform,\n",
    "    transform=test_image_transform\n",
    ")\n",
    "print(f\"[Test] Total video chunks of {SEQUENCE_LENGTH} frames: {len(test_dataset)}\")\n",
    "\n",
    "frames_tensor, labels_tensor = test_dataset[0]\n",
    "print(f\"Frames shape: {frames_tensor.shape}. Labels shape: {labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee54c3b2-28d1-456b-ab91-a573f99b5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        num_classes,\n",
    "        num_layers,\n",
    "        feature_extractor,\n",
    "        feature_extractor_dim,\n",
    "        device,\n",
    "        name\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.feature_extractor = self.feature_extractor.to(device=device)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_extractor_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.classifier = nn.Linear(\n",
    "            hidden_dim,\n",
    "            num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        # x: (B, T, C, H, W)\n",
    "        B, T, C, H, W = x.shape\n",
    "        features = []\n",
    "        for t in range(T):\n",
    "            with torch.no_grad():\n",
    "                f = self.feature_extractor(x[:, t])  # (B, C_feat, H', W')\n",
    "                f = f.view(B, -1)\n",
    "                features.append(f)\n",
    "                \n",
    "        features = torch.stack(features, dim=1)  # (B, T, C_feat, H', W')\n",
    "        if h:\n",
    "            _, (h, c) = self.lstm(features, (h, c))\n",
    "        else:\n",
    "            _, (h, c) = self.lstm(features)\n",
    "            \n",
    "        out = self.classifier(h[-1])\n",
    "        \n",
    "        return out, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c795d35-d65e-4936-956b-b748123b5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_lstm = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "efficientnet_lstm_dim = 1280\n",
    "efficientnet_name = \"EfficientNetB0_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e33653bf-2148-471d-a19b-8b15fb191c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobnet_convLSTM = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "mobnet_convLSTM_dim = 1280\n",
    "mobnet_name = \"MobileNetV2_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c604264-5c82-4f9a-9eff-b0ea2f3fec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_convLSTM = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "vgg16_convLSTM_dim = 512\n",
    "vgg16_name = \"VGG16_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25caaa-d5fb-4490-9269-dca00bf8b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define params grid\n",
    "batch_sizes = [ 32 ]\n",
    "device = \"cuda\"\n",
    "lr = 1e-3\n",
    "params = {\n",
    "    \"batches\": batch_sizes,\n",
    "    \"epochs\": [ 60 ],\n",
    "    \"layers\": [ 1 ],\n",
    "    \"hidden_state_dim\": [ 256 ]\n",
    "}\n",
    "model_names = ['MobileNetV2_lstm', 'EfficientNet_lstm', 'VGG16_lstm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524e138-92e3-4a36-a962-6692c614c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = list(product(model_names, *params.values()))\n",
    "best_results = {\n",
    "    'EfficientNet_lstm': {\n",
    "        'val_loss': float('inf'),\n",
    "        'params': None\n",
    "    },\n",
    "    'MobileNetV2_lstm': {\n",
    "        'val_loss': float('inf'),\n",
    "        'params': None\n",
    "    },\n",
    "    'VGG16_lstm': {\n",
    "        'val_loss': float('inf'),\n",
    "        'params': None\n",
    "    }\n",
    "}\n",
    "checkpoints_prefixes = dict()\n",
    "\n",
    "for i, (model_name, *combo_params) in enumerate(model_params, start=1):\n",
    "    if model_name == 'EfficientNet_lstm':\n",
    "        model = CnnExtractorConvLSTM(\n",
    "            num_classes=len(unique_labels),\n",
    "            hidden_channels_list=combo_params[3],\n",
    "            kernel_size=[(3,3) for _ in combo_params[3]],\n",
    "            device=\"cuda\",\n",
    "            feature_extractor=efficientnet_convLSTM.features,\n",
    "            feature_dim=efficientnet_convLSTM_dim,\n",
    "            name=model_name,\n",
    "            input_dropout=combo_params[2][0],\n",
    "            recurrent_dropout=combo_params[2][1],\n",
    "            output_dropout=combo_params[2][2]\n",
    "        ).to(device=\"cuda\")\n",
    "    if model_name == 'MobileNetV2_convLSTM':\n",
    "        model = CnnExtractorConvLSTM(\n",
    "            num_classes=len(unique_labels),\n",
    "            hidden_channels_list=combo_params[3],\n",
    "            kernel_size=[(3,3) for _ in combo_params[3]],\n",
    "            device=\"cuda\",\n",
    "            feature_extractor= mobnet_convLSTM.features,\n",
    "            feature_dim=mobnet_convLSTM_dim,\n",
    "            name=model_name,\n",
    "            input_dropout=combo_params[2][0],\n",
    "            recurrent_dropout=combo_params[2][1],\n",
    "            output_dropout=combo_params[2][2]\n",
    "        ).to(device=\"cuda\")\n",
    "    if model_name == 'VGG16_convLSTM':\n",
    "        model = CnnExtractorConvLSTM(\n",
    "            num_classes=len(unique_labels),\n",
    "            hidden_channels_list=combo_params[3],\n",
    "            kernel_size=[(3,3) for _ in combo_params[3]],\n",
    "            device=\"cuda\",\n",
    "            feature_extractor= vgg16_convLSTM.features,\n",
    "            feature_dim=vgg16_convLSTM_dim,\n",
    "            name=model_name,\n",
    "            input_dropout=combo_params[2][0],\n",
    "            recurrent_dropout=combo_params[2][1],\n",
    "            output_dropout=combo_params[2][2]\n",
    "        ).to(device=\"cuda\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=train_loss_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    checkpoint_prefix = hash(str(combo_params))\n",
    "    checkpoints_prefixes[f'{checkpoint_prefix}'] = combo_params\n",
    "    torch.save(checkpoints_prefixes, f\"/mnt/f/Checkpoints/Tesis/checkpoint_prefixes.pth\")\n",
    "    checkpoint_path = f\"/mnt/f/Checkpoints/Tesis/{model_name}_{checkpoint_prefix}.pth\"\n",
    "    checkpoint_path_best = f\"/mnt/f/Checkpoints/Tesis/{model_name}_{checkpoint_prefix}_best.pth\"\n",
    "    \n",
    "    train_dataset = ExoNetDataset(\n",
    "        df_labels=seq_train_df,\n",
    "        img_dir=DATASET_DIR,\n",
    "        seq_len=SEQUENCE_LENGTH,\n",
    "        target_transform=encoder.transform,\n",
    "        transform=train_image_transform\n",
    "    )\n",
    "    val_dataset = ExoNetDataset(\n",
    "        df_labels=seq_val_df,\n",
    "        img_dir=DATASET_DIR,\n",
    "        seq_len=SEQUENCE_LENGTH,\n",
    "        target_transform=encoder.transform,\n",
    "        transform=val_image_transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=combo_params[0],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=combo_params[0],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model, history, val_loss = train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        checkpoint_path_best=checkpoint_path_best,\n",
    "        epochs=combo_params[1],\n",
    "        device=\"cuda\",\n",
    "        n_processes=len(model_params),\n",
    "        i_process=i\n",
    "    )\n",
    "    if val_loss < best_results[model_name]['val_loss']:\n",
    "        best_results[model_name]['val_loss'] = val_loss\n",
    "        best_results[model_name]['params'] = combo_params\n",
    "        \n",
    "    torch.save(best_results, f\"/mnt/f/Checkpoints/Tesis/best_results.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5f1392e-eb52-42db-b824-bffd1ecc9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    checkpoint_path,\n",
    "    checkpoint_path_best,\n",
    "    epochs=10,\n",
    "    device=\"cuda\",\n",
    "    n_processes=1,\n",
    "    i_process=1\n",
    "):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_weights = None\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        loop = tqdm(train_dataloader, total=len(train_dataloader), leave=True)\n",
    "        loop.set_description(f\"Epoch [{epoch}/{epochs}] - Proc: {i_process}/{n_processes}\")\n",
    "        \n",
    "        for i, (xb, yb) in enumerate(loop, start=1):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device).long()\n",
    "    \n",
    "            optimizer.zero_grad() # zero the parameter gradients\n",
    "            logits, _, _ = model(xb)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
    "            loss.backward()\n",
    "    \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # gradient clipping\n",
    "            optimizer.step() # update weights\n",
    "    \n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "    \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.numel()\n",
    "            loop.set_postfix(loss=(running_loss/len(loop)), acc=f\"{(correct/total):.4f}\")\n",
    "                \n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        epoch_acc = correct / total\n",
    "        history[\"train_loss\"].append(epoch_loss)\n",
    "        history[\"train_acc\"].append(epoch_acc)\n",
    "        \n",
    "        save_state(checkpoint_path, model, optimizer, epoch, history)\n",
    "    \n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_len = len(val_dataloader)\n",
    "        with torch.no_grad():\n",
    "            val_loop = tqdm(val_dataloader, total=len(val_dataloader), leave=True)\n",
    "            val_loop.set_description(\"Validation\")\n",
    "            \n",
    "            for j, (xb, yb) in enumerate(val_loop, start=1):\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device).long()\n",
    "    \n",
    "                logits, _, _ = model(xb)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "    \n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_total += yb.numel()\n",
    "    \n",
    "                val_loop.set_postfix(loss=val_loss/len(val_loop), acc=f\"{(val_correct/val_total):.4f}\")       \n",
    "        \n",
    "        val_loss = val_loss / val_len\n",
    "        val_acc = val_correct / val_total\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        if val_loss < best_val_loss:\n",
    "            save_state(checkpoint_path_best, model, optimizer, epoch, history)\n",
    "            best_val_loss = val_loss\n",
    "            best_weights = model.state_dict()\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model, history, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8883e04a-536a-46a7-bbdb-fdecee7e982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512, 32])\n",
      "torch.Size([10, 512, 224]) torch.Size([2, 10, 224]) torch.Size([2, 10, 224])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10,512,32)\n",
    "print(x.shape)\n",
    "lstm = nn.LSTM(\n",
    "            input_size=(32),\n",
    "            hidden_size=224,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "output, (h, c) = lstm(x)\n",
    "print(output.shape, h.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd4562-fbb3-43d5-87a7-b2ba41c88d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn_model(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    criterion,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_len = len(test_dataloader)\n",
    "    with torch.no_grad():\n",
    "        test_loop = tqdm(test_dataloader, total=len(test_dataloader), leave=True)\n",
    "        test_loop.set_description(\"Test\")\n",
    "        \n",
    "        for j, (xb, yb) in enumerate(test_loop, start=1):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device).long()\n",
    "    \n",
    "            logits, _, _ = model(xb)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
    "            test_loss += loss.item() * xb.size(0)\n",
    "    \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            test_correct += (preds == yb).sum().item()\n",
    "            test_total += yb.numel()\n",
    "    \n",
    "            test_loop.set_postfix(loss=test_loss/len(test_loop), acc=f\"{(test_correct/test_total):.4f}\")\n",
    "    \n",
    "    test_loss = test_loss / test_len\n",
    "    test_acc = test_correct / test_total\n",
    "    history[\"test_loss\"].append(test_loss)\n",
    "    history[\"test_acc\"].append(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
